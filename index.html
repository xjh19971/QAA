<!DOCTYPE html>
<html>

<head>
  <link rel="icon" href="assets/title_image.png" type="image/x-icon">

  <meta charset="utf-8">
  <meta name="description"
    content="Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition">
  <meta name="keywords" content="Visual Place Recongnition, Multi-Dataset Training">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition
  </title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://xjh19971.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Related Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://xjh19971.github.io/VG-SSL/">
              VG-SSL
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Query-Based Adaptive Aggregation for Multi-Dataset Joint Training
              Toward Universal Visual Place Recognition</h1>
            <h2 class="subtitle is-4 publication-venue"> IEEE International Conference on Robotics & Automation (ICRA)
              2026 </h2>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="https://xjh19971.github.io">Jiuhong Xiao</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://yangrobotics.com/">Yang Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://engineering.nyu.edu/faculty/giuseppe-loianno">Giuseppe Loianno</a><sup>2</sup>,</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>New York University</a>,
                <sup>2</sup>University of California, Berkeley</a>
            </div>

            <div class="column has-text-centered">
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2507.03831" class="external-link button is-normal ">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://youtu.be/i_PAvzXKul4"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/xjh19972/query-based-adaptive-aggregation-qaa-models"
                  class="external-link button is-normal">
                  <span class="icon">
                    <i class="fa fa-face-smiling-hands"></i>
                  </span>
                  <span>HF Models</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/arplaboratory/QAA" class="external-link button is-normal ">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="notification is-info is-light" style="margin-bottom: 0;">
              <h2 class="title is-3">TL;DR</h2>
              <p class="is-size-5">
                QAA introduces a novel feature aggregation technique using learned queries as an <strong>extensive
                  reference codebook</strong> to enhance model capacity, enabling effective multi-dataset joint training
                for Visual Place Recognition.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/i_PAvzXKul4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>      </div>
      </div>
    </div>
    </div>
</section> -->



  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Deep learning methods for Visual Place Recognition (VPR) have advanced significantly, largely driven by
                large-scale datasets. However, most existing approaches are trained on a single dataset, which can
                introduce dataset-specific inductive biases and limit model generalization. While multi-dataset joint
                training offers a promising solution for developing universal VPR models, divergences among training
                datasets can saturate the limited information capacity in feature aggregation layers, leading to
                suboptimal performance. To address these challenges, we propose Query-based Adaptive Aggregation (QAA),
                a novel feature aggregation technique that leverages learned queries as reference codebooks to
                effectively enhance information capacity without significant computational or parameter complexity. We
                show that computing the Cross-query Similarity (CS) between query-level image features and reference
                codebooks provides a simple yet effective way to generate robust descriptors. Our results demonstrate
                that QAA outperforms state-of-the-art models, achieving balanced generalization across diverse datasets
                while maintaining peak performance comparable to dataset-specific models. Ablation studies further
                explore QAA's mechanisms and scalability. Visualizations reveal that the learned queries exhibit diverse
                attention patterns across datasets. Code and models are publicly available.
              </p>
            </div>
          </div>
        </div>
      </div>
  </section>

  <!-- <section class="hero is-light">
<div class="hero-body">
  <div class="columns is-centered has-text-centered">
    <div class="column">
      <h2 class="title is-3" >UASTHN Detected Failure Cases</h2>
      <p></p>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Input</h2>
          <img id="sgm_1" src="./data/sgm/image35.gif" width="400">
          <img id="sgm_2" src="./data/sgm/image42.gif" width="400">
          <img id="sgm_3" src="./data/sgm/image36.gif" width="400">
          <img id="sgm_4" src="./data/sgm/image39.gif" width="400">
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Baseline</h2>
          <img id="sgm_5" src="./data/sgm/image31.gif" width="400">
          <img id="sgm_6" src="./data/sgm/image32.gif" width="400">
          <img id="sgm_7" src="./data/sgm/image33.gif" width="400">
          <img id="sgm_8" src="./data/sgm/image34.gif" width="400">
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Ours</h2>
          <img id="sgm_9" src="./data/sgm/image37.gif" width="400">
          <img id="sgm_6" src="./data/sgm/image38.gif" width="400">
          <img id="sgm_7" src="./data/sgm/image40.gif" width="400">
          <img id="sgm_8" src="./data/sgm/image41.gif" width="400">
        </div>
      </div>
    </div>
  </div>
</div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column">
      <h2 class="title is-3">TGM Generated Thermal Images</h2>
      <p></p>
    </div>
  </div> -->



  <!-- <section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column">
      <h2 class="title is-3">Poster Presentation</h2>
    </div>
  </div>
<div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <div class="content">
          <iframe src="data/poster_VG_SSL.pdf" width="100%" height="600px" style="border:none;">
        </iframe>
        </div>
      </div>
    </div>
  </div>
</section> -->

  <!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>
    @inproceedings{xiao2025vg,
      title={VG-SSL: Benchmarking Self-Supervised Representation Learning Approaches for Visual Geo-Localization},
      author={Xiao, Jiuhong and Zhu, Gao and Loianno, Giuseppe},
      booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
      pages={6667--6677},
      year={2025},
      organization={IEEE}
    }
    </code></pre>
  </div>
</section> -->

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
          <img alt="Creative Commons License" style="border-width:0"
            src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website adapted from the Nerfies templates, which is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                International License</a>.
            </p>
            <p>
              If you use the <a href="https://xjh19971.github.io/STHN/">source code</a> of this website,
              please also link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source
                code</a> in your footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>